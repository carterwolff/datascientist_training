---
title: 'Supervised Learning in R: Regression'
author: "Carter Wolff"
date: "2023-07-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---Chapter 1: What is Regression
Advantages and Limitations to Linear Regression
-Pros: 
  -easy to fit
  -concise
  -less prone to overfitting
  -interpretable
-Cons:
  -cannot express nonlinear relationships
  -collinearity - when two or more predictors are correlated with one each other

---Chapter 2: Training and Evaluating Regression Models
Evaluating a Model Graphically
-The Gain Curve Plot is similar to a roc curve in view the performance of a model. 
  -The gain curve is found in WVPlots package in R and is accessed with GainCurvePlot
    -GainCurvePlot(dataframe_name, prediction_column, outcome_column, model_name)

Root Mean Squared Error
-RMSE can be compared to the standard deviation of the outcome variable of the model. If the RMSE is less than the sd, the model is better at predicting than if you used the average alone

Properly Training a Model
-It is always good practice to apply your model to data that was not used to build it (test data). Normally, you split your data set up into train data and test data, but  if you don't have enough data, you can opt for cross validation
-Cross validation splits your data up n different ways. It then uses those subgroups as test and train data. For n = 3, Groups A and B are used to train the data, while group C is used ot test the model. Do this for each combination of test/train data groups.
-The cross validation package, vtreat, uses the function kWayCrossValidation() to create a plan to split data. kWayCrossValidation() uses a few arguments
  -# of rows in your data frame
  -# of splits: i.e., how training data sets will be predicted on
-See the code below for splitting training and test data AND how to create cross validation plans. The data, mpg is available from ggplot

```{r}
library(ggplot2)
# mpg is available
summary(mpg)
dim(mpg)

# Use nrow to get the number of rows in mpg (N) and print it
(N <- nrow(mpg))

# Calculate how many rows 75% of N should be and print it
# Hint: use round() to get an integer
(target <- round(N * 0.75))

# Create the vector of N uniform random variables: gp
gp <- runif(N)

# Use gp to create the training set: mpg_train (75% of data) and mpg_test (25% of data)
mpg_train <- mpg[gp < 0.75, ]
mpg_test <- mpg[gp >= 0.75, ]

# Use nrow() to examine mpg_train and mpg_test
nrow(mpg_train)
nrow(mpg_test)

# mpg_train is available
summary(mpg_train)

# Create a formula to express cty as a function of hwy: fmla and print it.
(fmla <- as.formula("cty ~ hwy"))

# Now use lm() to build a model mpg_model from mpg_train that predicts cty from hwy 
mpg_model <- lm(fmla, data = mpg_train)

# Use summary() to examine the model
summary(mpg_model)


###k Fold Cross Validation with vtreat package###
# Load the package vtreat
library(vtreat)

# mpg is available
summary(mpg)

# Get the number of rows in mpg
nRows <- nrow(mpg)

# Implement the 3-fold cross-fold plan with vtreat
splitPlan <- kWayCrossValidation(nRows, 3, NULL, NULL)

# Examine the split plan
str(splitPlan)

# Run the 3-fold cross validation plan from splitPlan
k <- 3 # Number of folds
mpg$pred.cv <- 0 
for(i in 1:k) {
  split <- splitPlan[[i]]
  model <- lm(cty ~ hwy, data = mpg[split$train, ])
  mpg$pred.cv[split$app] <- predict(model, newdata = mpg[split$app, ])
}

# Predict from a full model
mpg$pred <- predict(lm(cty ~ hwy, data = mpg))

# Get the rmse of the full model's predictions
rmse(mpg$pred, mpg$cty)

# Get the rmse of the cross-validation predictions
rmse(mpg$pred.cv, mpg$cty)
```


---Chapter 3: Issues to Consider
Categorical Inputs
-R represents categorical inputs with dummy variables. The more levels, the more dummy codes are required
  -in any case, there is always a reference level that R uses
-model.matrix allows you to see the code for the categorical variable, along with the reference level
-for R, this process is called one-hot encoding
-R creates a coefficient for each of the levels 

-The code below compares two linear models of houseprice ~ size. One model uses size while the second uses size ^ 2. This code also uses three fold cross validation given the data set is small

```{r}
library(ggplot2)
library(vtreat)
library(dplyr)
library(tidyr)

houseprice <- readRDS("houseprice.rds")

summary(houseprice)

fmla_sqr <- as.formula("price ~ I(size^2)")

# Create a splitting plan for 3-fold cross validation
set.seed(34245)  # set the seed for reproducibility
splitPlan <- kWayCrossValidation(nrow(houseprice), 3, NULL, NULL)

# Sample code: get cross-val predictions for price ~ size
houseprice$pred_lin <- 0  # initialize the prediction vector
for(i in 1:3) {
  split <- splitPlan[[i]]
  model_lin <- lm(price ~ size, data = houseprice[split$train,])
  houseprice$pred_lin[split$app] <- predict(model_lin, newdata = houseprice[split$app,])
}

# Get cross-val predictions for price as a function of size^2 (use fmla_sqr)
houseprice$pred_sqr <- 0 # initialize the prediction vector
for(i in 1:3) {
  split <- splitPlan[[i]]
  model_sqr <- lm(fmla_sqr, data = houseprice[split$train, ])
  houseprice$pred_sqr[split$app] <- predict(model_sqr, newdata = houseprice[split$app, ])
}

# Gather the predictions and calculate the residuals
houseprice_long <- houseprice %>%
  gather(key = modeltype, value = pred, pred_lin, pred_sqr) %>%
  mutate(residuals = price - pred)

# Compare the cross-validated RMSE for the two models
houseprice_long %>% 
  group_by(modeltype) %>% # group by modeltype
  summarize(rmse = sqrt(mean(residuals^2)))
```

---Chapter 4: Dealing with Non-Linear Responses
Logistic Regression to Predict Probabilities
-In this exercise, we use a data set called sparrow, which records the status of sparrow survival as survived or perished
  -the data set also includes predictor variables of total_length, weight, and humerus
-We will build a logistic regression to determine how these factors predict the probability of survival in sparrows

```{r}
library(ggplot2)
library(vtreat)
library(dplyr)
library(tidyr)
library(broom)
library(WVPlots)

sparrow <- readRDS("sparrow.rds")

# sparrow is available
summary(sparrow)

# Create the survived column - converting status to a TRUE/FALSE response
sparrow$survived <- sparrow$status == "Survived"

# Create the formula
(fmla <- as.formula("survived ~ total_length + weight + humerus"))

# Fit the logistic regression model
sparrow_model <- glm(fmla, data = sparrow, family = "binomial")

# Call summary
summary(sparrow_model)

# Call glance
(perf <- glance(sparrow_model))

# Calculate pseudo-R-squared: pseudo-R-squared is similar to R squared for lm. The value is 1 - deviance/null deviance
(pseudoR2 <- 1 - perf$deviance/perf$null.deviance)

# Make predictions with the logistic model
sparrow$pred <- predict(sparrow_model, type = "response")

# Look at gain curve: Gain Curves compare predicted values with actual outcomes to see how the model performs.
# Our model appears in blue and the perfect model outcome (aka Wizard Curve) is show in green
GainCurvePlot(sparrow, "pred", "survived", "sparrow survival model")

```
Regression to Predict Count Data - Poisson and Quasipoisson regression
-Poisson distribution assumes that the mean(y) is close to var(y). If this is not the case, a quasipoisson distribution is more appropriate
  -by "close to" a good rule of thumb is that the mean and variance are within the same order of magnitude
-In either case, these regressions should be done with large sets of data
-The code below will evaluate bike rental rates assuming a poisson or quasipoisson regression approach

```{r}
load("C:/Users/carte/Desktop/Data Science/R/Bikes.RData")
str(bikesJuly)

outcome = "cnt"
vars = c("hr", "holiday", "workingday", "weathersit", "temp", "atemp", "hum", "windspeed")

# Create the formula string for bikes rented as a function of the inputs
(fmla <- paste(outcome, "~", paste(vars, collapse = " + ")))

# Calculate the mean and variance of the outcome - mean and var differ, thus quasipoisson should be used
(mean_bikes <- mean(bikesJuly$cnt))
(var_bikes <- var(bikesJuly$cnt))

# Fit the model
bike_model <- glm(fmla, data = bikesJuly, family = "quasipoisson")

# Call glance
(perf <- glance(bike_model))

# Calculate pseudo-R-squared
(pseudoR2 <- 1 - perf$deviance/perf$null.deviance)

#using bikesJuly model to predict bike rentals of bikeAugust
str(bikesAugust)

# bike_model is available
summary(bike_model)

# Make predictions on August data
bikesAugust$pred  <- predict(bike_model, newdata = bikesAugust, type = "response")

# Calculate the RMSE
bikesAugust %>% 
  mutate(residual = cnt - pred) %>%
  summarize(rmse  = sqrt(mean(residual^2)))

# Plot predictions vs cnt (pred on x-axis)
ggplot(bikesAugust, aes(x = pred, y = cnt)) +
  geom_point() + 
  geom_abline(color = "darkblue")

#visualizine bikerental predictions as a function of time
# Plot predictions and cnt by date/time
bikesAugust %>% 
  # set start to 0, convert unit to days
  mutate(instant = (instant - min(instant))/24) %>%  
  # gather cnt and pred into a value column
  tidyr::gather(key = valuetype, value = value, cnt, pred) %>%
  filter(instant < 14) %>% # restric to first 14 days
  # plot value by instant
  ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous("Day", breaks = 0:14, labels = 0:14) + 
  scale_color_brewer(palette = "Dark2") + 
  ggtitle("Predicted August bike rentals, Quasipoisson model")
```
GAM to learn non-linear transforms
-Generalized additive models are used to build nonlinear models. The gam() function in the mgcv package takes similar arguments to R's base model functions
- you can explicitly tell R to treat a model formula as nonlinear by wrapping the explanatory variables in the s() function
    -i.e., y ~ s(x)
    -s should not be used with categorical variables
-R code for build gams below
```{r}
load("C:/Users/carte/Desktop/Data Science/R/Bikes.RData")
library(mgcv)

# soybean_train is available
summary(soybean_train)

# Plot weight vs Time (Time on x axis)
ggplot(soybean_train, aes(x = Time, y = weight)) + 
  geom_point()

# Load the package mgcv
library(mgcv)

# Create the formula 
(fmla.gam <- as.formula("weight ~ s(Time)"))

# Fit the GAM Model
model.gam <- gam(fmla.gam, data = soybean_train, family = gaussian)

#Fit linear model to compare to GAM model
model.lin <- lm(weight ~ Time, data = soybean_train)

# From previous step
library(mgcv)
fmla.gam <- weight ~ s(Time)
model.gam <- gam(fmla.gam, data = soybean_train, family = gaussian)

# Call summary() on model.lin and look for R-squared
summary(model.lin)

# Call summary() on model.gam and look for R-squared
summary(model.gam)

# Call plot() on model.gam
plot(model.gam)

# soybean_test is available
summary(soybean_test)

# Get predictions from linear model
soybean_test$pred.lin <- predict(model.lin, newdata = soybean_test)

# Get predictions from gam model
soybean_test$pred.gam <- as.numeric(predict(model.gam, newdata = soybean_test))

# Gather the predictions into a "long" dataset
soybean_long <- soybean_test %>%
  gather(key = modeltype, value = pred, pred.lin, pred.gam)

# Calculate the rmse
soybean_long %>%
  mutate(residual = weight - pred) %>%     # residuals
  group_by(modeltype) %>%                  # group by modeltype
  summarize(rmse = sqrt(mean(residual ^ 2))) # calculate the RMSE

# Compare the predictions against actual weights on the test data
soybean_long %>%
  ggplot(aes(x = Time)) +                          # the column for the x axis
  geom_point(aes(y = weight)) +                    # the y-column for the scatterplot
  geom_point(aes(y = pred, color = modeltype)) +   # the y-column for the point-and-line plot
  geom_line(aes(y = pred, color = modeltype, linetype = modeltype)) + # the y-column for the point-and-line plot
  scale_color_brewer(palette = "Dark2")
  
```


---Chapter 5: Tree-Based Methods
Tree-based Methods
-Trees operate on categorization of data. Therefore, they are unable to make continuous predictions like a linear model can. 
-Trees are also susceptible to too few or too man splits.
  -With too few splits, the tree is called a shallow tree and is only capable of making coarse-grain predictions
  -With too many splits the tree is called a deep tree and is prone to overfitting the training data

Random Forests
-The code below uses a random forest approach, with the package ranger() to predict bike rentals based on weather variables
-using predict() with a random forest model returns a list. One of the elements of the list is predictions which is similar to the standard predict() returns in previous models
  -thus in the code below, we specific this element of the list to return. That way we get a vector that we can attach to our test data set

```{r}
# bikesJuly is available
str(bikesJuly)

# Random seed to reproduce results - will be called in the ranger function later
seed

# The outcome column
(outcome <- "cnt")

# The input variables
(vars <- c("hr", "holiday", "workingday", "weathersit", "temp", "atemp", "hum", "windspeed"))

# Create the formula string for bikes rented as a function of the inputs
(fmla <- paste(outcome, "~", paste(vars, collapse = " + ")))

# Load the package ranger
library(ranger)

# Fit and print the random forest model
(bike_model_rf <- ranger(fmla, # formula 
                         bikesJuly, # data
                         num.trees = 500, 
                         respect.unordered.factors = "order", 
                         seed = seed))

# bikesAugust is available as a test data set
str(bikesAugust)

# Make predictions on the August data
bikesAugust$pred <- predict(bike_model_rf, bikesAugust)$predictions

# Calculate the RMSE of the predictions
bikesAugust %>% 
  mutate(residual = cnt - pred)  %>% # calculate the residual
  summarize(rmse  = sqrt(mean(residual^2)))      # calculate rmse

# Plot actual outcome vs predictions (predictions on x-axis)
ggplot(bikesAugust, aes(x = pred, y = cnt)) + 
  geom_point() + 
  geom_abline()

##Plotting predictions and actual bike rentals for the first two weeks of August as a function of time
first_two_weeks <- bikesAugust %>% 
  # Set start to 0, convert unit to days
  mutate(instant = (instant - min(instant)) / 24) %>% 
  # Gather cnt and pred into a column named value with key valuetype
  gather(key = valuetype, value = value, cnt, pred) %>%
  # Filter for rows in the first two
  filter(instant < 14) 

# Plot predictions and cnt by date/time 
ggplot(first_two_weeks, aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous("Day", breaks = 0:14, labels = 0:14) + 
  scale_color_brewer(palette = "Dark2") + 
  ggtitle("Predicted August bike rentals, Random Forest plot")
```

One-Hot-Encoding Categorical Variables
-xgboost() does not use categorical variables inherently. Thus, we need to code them for use in these types of algorithms
-the designTreatmentsZ() from the vtreat package creates a treatment plan for the training data, while the prepare() function will remove NAs and shift categorical variables to numerical values
-inputs of designTreatmentsZ(dframe, varslist, verbose = FALSE)
  -dframe: Training data
  -varslist: list of input variable names
  -verbose = False will suppress progress mesages
designTreatmentZ returns an element known as score frame, which can be use to retrieve the new encoded information about each categorical variable level
  -treatplan <- designTreatmentsZ(dframe, varslist, verbose = FALSE)
  -scoreFrame <- treatplan$scoreFrame %>% select(varName, origName, code)
  -newvars <- scoreFrame %>% filter(code in %in% c("clean", "lev")) %>% use_series(varName)
  The three lines of code above will return the levels found in the initial dataset, after one-hot encodeing takes effect
  -now call prepare to get the treated dataframe
  prepare(treatmentplan #from the step above#, dateframe, varRestriction = newvars)
-The code below properly cleans and one-hot encodes the data from the bike rentals for both July and August

```{r}
library(magrittr)

# The outcome column
(outcome <- "cnt")

# The input columns
(vars <- c("hr", "holiday", "workingday", "weathersit", "temp", "atemp", "hum", "windspeed"))

# Load the package vtreat
library(vtreat)

# Create the treatment plan from bikesJuly (the training data)
treatplan <- designTreatmentsZ(bikesJuly, vars, verbose = FALSE)

# Get the "clean" and "lev" variables from the scoreFrame
(newvars <- treatplan %>%
  use_series(scoreFrame) %>%        
  filter(code %in% c("clean", "lev")) %>%  # get the rows you care about
  use_series(varName))           # get the varName column

# Prepare the training data
bikesJuly.treat <- prepare(treatplan, bikesJuly,  varRestriction = newvars)

# Prepare the test data
bikesAugust.treat <- prepare(treatplan, bikesAugust,  varRestriction = newvars)

# Call str() on the treated data
str(bikesJuly.treat)
str(bikesAugust.treat)
```

Gradient Boosting Machines
-Ensemble method that continuously iterates on the previous model to find a better fit. 
-xgboost is prone to overfitting if you specify enough trees. Thus, it is a common approach to estimate the the Root Mean Square Error through cross validation and find the number of trees in xgboost that satisfies that amount. 
  -To do this, there is a function in the xgboost package called xgb.cv() it takes in numerous arguments, shown below
    -The overall syntax of xgb.cv is xgb.cv(data = , label = , nrounds = , nfolds = , objective = , eta = , max_depth = , early_stopping_rounds = , verbose = FALSE)
      -data = numeric matrix
      -label = vector of outcomes
      -nrounds = the maximum number of rounds (i.e, max trees to build)
      -nfold = the number of folds for the cross-validation
      -obejective = "reg:squarederror" which is the evaluation for continuous outcomes
      -eta = the learning rate, between 0 and 1. closer to 1 equals faster learning, but more prone to overfitting
      -max_depth = maximum depth of the trees
      -early_stopping_roudns: is you see this many rounds without improvement, stop the run
      -verbose = FALSE to suppress progress updates
      
-see the code below for using xgboost to predict bike rentals data using xgboost()
```{r}
set.seed(1234)

# Load the package xgboost
library(xgboost)

# Run xgb.cv
cv <- xgb.cv(data = as.matrix(bikesJuly.treat), 
            label = bikesJuly$cnt,
            nrounds = 50,
            nfold = 5,
            objective = "reg:squarederror",
            eta = 0.75,
            max_depth = 5,
            early_stopping_rounds = 5,
            verbose = FALSE   # silent
)

# Get the evaluation log 
elog <- cv$evaluation_log

# Determine and print how many trees minimize training and test error
elog %>% 
   summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min(train_rmse_mean)
             ntrees.test  = which.min(test_rmse_mean))   # find the index of min(test_rmse_mean)
ntrees <- 30 #take from the elog code above

# Run xgboost with nrounds set to ntrees
bike_model_xgb <- xgboost(data = as.matrix(bikesJuly.treat), # training data as matrix
                   label = bikesJuly$cnt,  # column of outcomes
                   nrounds = ntrees,       # number of trees to build
                   objective = "reg:squarederror", # objective
                   eta = 0.75,
                   max_depth = 5,
                   verbose = FALSE  # silent
)

# Make predictions
bikesAugust$pred <- predict(bike_model_xgb, as.matrix(bikesAugust.treat))

# Plot predictions (on x axis) vs actual bike rental count
ggplot(bikesAugust, aes(x = pred, y = cnt)) + 
  geom_point() + 
  geom_abline()

# Calculate RMSE
bikesAugust %>%
  mutate(residuals = cnt - pred) %>%
  summarize(rmse = sqrt(mean(residuals^2)))

# Plot predictions and actual bike rentals as a function of time (days)
bikesAugust %>% 
  mutate(instant = (instant - min(instant))/24) %>%  # set start to 0, convert unit to days
  gather(key = valuetype, value = value, cnt, pred) %>%
  filter(instant < 14) %>% # first two weeks
  ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + 
  geom_point() + 
  geom_line() + 
  scale_x_continuous("Day", breaks = 0:14, labels = 0:14) + 
  scale_color_brewer(palette = "Dark2") + 
  ggtitle("Predicted August bike rentals, Gradient Boosting model")
```

